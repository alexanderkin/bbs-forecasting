# Title

Authors: Harris, Taylor and White?

## Introduction

Quality of predictions is important:
  * helps determine policy/set priorities
  * something something climate change
  * need to know both accuracy of point estimates and how reasonable our prediction intervals are.
  * Overconfident predictions can be worse than uncertainty
  * Quality of prediction is one metric for how much we know about ecological processes
Unclear how much to trust existing predictions:
  * Strong assumptions (see "misc" below)
  * many *interesting* predictions
  * Potential problems with confidence intervals
  * Potential disconnect between training target and actual goals
  * Difficult to test model assumptions/predictions because of time scales
    * Most scientists making predictions today will have retired by 2050
Demonstrate best practices \& their contribution to better predictions
  * Use long-term data sets such as BBS to evaluate predictive skill
  * Note the large change in average temperatures since the 80's; comparable to the changes expected in the next N years
  * Lots of talk about how we should do it, fewer examples of doing them all together
  * Examples of best practices
    * Not just bioclim
    * Dynamic predictor variables (not just site-level averages over 50 years like 
      Worldclim)
    * time series models (temporal autocorrelation)
    * spatial/site-level stuff (repeated measures aren't fully independent)
    * Properly handling uncertainty:
      * probabilistic/error bars
      * error propagation
    * multiple models
      * Test with simple baselines (e.g. random walk, future-is-like-present)
  * Focus on richness/diversity because \[citations saying that it's important\]
  * Publish forecasts at different timescales to evaluate performance/credibility
Here we do that:
  * forecasts for biodiversity in NAmerican birds with as many best practices as possible
  * Discuss the influence of these improved methods \& of our error rates on our ability to understand:
    * How to improve
    * How much to trust the predictions

## Methods

**Data**

* BBS + retriever
* Monthly climate data from interpolated weather stations (PRISM)
* NDVI (GIMMS)
* Altitude (SRTM 90m Digital Elevation Database via raster::getData)

**Models**

* Mixed model for observer effort \& site effects: used lme4, included random
  effects observer_id to account for situations were one observer consistently
  found more (or fewer) species than average. Observer effects are inherent in
  large data sets taken by different observers, and are known to occur in BBS 
  [citation]. These models also included a site-level random effect to account
  for differences in actual richness across BBS routes. [[Maybe year effect?]].
  No fixed effects because not interested in explaining differences among sites
  at this point; just want to account for their existence.
* Residuals from this model give an estimate of how richness changed over time
  at each site, accounting for observer differences.  We then used this "corrected"
  version of the data for our time-series models such as the AR1, etc.
* Naive AR1 model (adjusted for observer effects). This model says that richness 
  can diffuse up or down from the previous year's value, without any constraints.
  As a result, the confidence intervals expand quickly over time.
* Average (adjusted for observer effects). This model says that richness at a site
  fluctuates around a fixed mean value, so its confidence intervals do not expand
  as the time since last observation increases.
* Auto-ARIMA (Hyndman 2016). Choose a differencing order, then do forward stepwise
  model selection based on AIC.
* Auto-ARIMA + environment.
* SDMs: use graf model w/ mixed model estimates of site \& observer effects
  as predictors (include error in predictors as estimated from the mixed model
  to propagate our uncertainty forward). This is basically SDM, but with a 
  better model (graf) and more information (monthly climate data instead of using constant values from the Worldclim data, and observer effects). We used Poisson-binomial confidence intervals, as discussed in Calabrese et al. 2014.
* richness-level GBM w/ mixed model point estimates as bonus predictors where 
  available. This lets us predict richness directly instead of via noisy 
  species-level estimates, but doesn't use the full data set in one model.
  Maybe need to run multiple times to propagate uncertainty from lme4.
* JSDM. Should get best of the previous 2. Might be able to propagate uncertainty
  from lme4.
* Ensembles. Averaging predictions from multiple models tends to reduce the 
  noise in the estimates, and can thus lead to better predictions. Choosing the
  weights for the ensemble isn't straightforward because our estimates of model 
  error on the training set are biased in favor of models that
  overfit. [[Another option is to spatially cross-validate on the training set]]

(need a side note about why we're not using a proper count distribution; underdispersion, easier access to out-of-the-box time series methods)

**Evaluation**

* (R)MSE
* Log-likelihood (simultaneously rewards good point estimates, precision, and coverage)
* Coverage at 95%
* Calibration/bias
* Evaluated each year after the last training year (2010?)
* Dig around in the residuals for insights

## Results and conclusions

* None of the models actually work all that well, compared to baseline.
* Temporal richness variability doesn't track the predictors we use; sites have "inertia"
  * Cite 3 papers Ethan found on consistancy of alpha even when composition changes dramatically
  * Good for absolute accuracy, little room for improvement over baseline
  * Maybe richness is too coarse
    * abundance
    * species-level
* More important to use a reasonable error model than to get the best predictors \& basis expansions
* Ensembles help/don't help (depending on results)
* Point estimate error versus coverage
* In many ways, this is the best-case scenario for SDMs:
  * large data set, known \& consistent sampling technique over a period of decades
  * "True" absences
  * extended predictor set
  * birds arguably migrate quickly enough to make equlibrium \& especially space-for-time assumptions less ridiculous
  * errors for individual species can cancel out; only asking for aggregate values
* Value of stacking versus predicting the quantitity of interest directly
* Need to start doing this, even if we're bad at it.
  * We've posted predictions for the next N years of BBS data, using several methods, and invite others to do so as well.


## misc

SDM assumptions:
  * we've measured all the important variables (sites don't have any other important properties)
  * Something like stationarity:
    * e.g. "systems are at equilibrium now and will be at equilibrium in future"
    * or "species track predictor variables quickly" (no lags)
    * etc
  * independent/identically distributed observations across species, years
  * Space substitutes well for time
