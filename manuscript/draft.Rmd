# Title

Authors: Harris, Taylor and White?

## Introduction

Quality of predictions is important:
  * helps determine policy/set priorities
  * something something climate change
  * need to know both accuracy of point estimates and how reasonable our prediction intervals are.
  * Overconfident predictions can be worse than uncertainty
  * Quality of prediction is one metric for how much we know about ecological processes
Unclear how much to trust existing predictions:
  * Strong assumptions (see "misc" below)
  * many *interesting* predictions
  * Potential problems with confidence intervals
  * Potential disconnect between training target and actual goals
  * Difficult to test model assumptions/predictions because of time scales
    * Most scientists making predictions today will have retired by 2050
Demonstrate best practices \& their contribution to better predictions
  * Use long-term data sets such as BBS to evaluate predictive skill
  * Note the large change in average temperatures since the 80's; comparable to 
    the potential changed in the next 20 years 
  * Lots of talk about how we should do it, fewer examples of doing them all together
  * Examples of best practices
    * Not just bioclim
    * Dynamic predictor variables (not just site-level averages over 50 years like 
      Worldclim)
    * time series models (temporal autocorrelation)
    * spatial/site-level stuff (repeated measures aren't fully independent)
    * Properly handling uncertainty:
      * probabilistic/error bars
      * error propagation
    * multiple models
      * Test with simple baselines (e.g. random walk, future-is-like-present)
  * Focus on richness/diversity because \[citations saying that it's important\]
  * Publish forecasts at different timescales to evaluate performance/credibility
Here we do that:
  * forecasts for biodiversity in NAmerican birds with as many best practices as 
    possible
  * Discuss the influence of these improved methods \& of our error rates on our 
    ability to understand:
    * How to improve
    * How much to trust the predictions

## Methods

### Data

* BBS + retriever
* Monthly climate data from interpolated weather stations (PRISM)
* NDVI (GIMMS)
* Altitude (SRTM 90m Digital Elevation Database via raster::getData)

### Accounting for site \& observer effects
* Observer effects are inherent in
  large data sets taken by different observers, and are known to occur in BBS 
  [citation].
* Mixed model for observer effort \& site effects: used Stan, included random
  effects observer_id to account for situations were one observer consistently
  found more (or fewer) species than average. These models also included a 
  site-level random effect to account for differences in actual richness across 
  BBS routes. [[Maybe year effect?]].
  No fixed effects because not interested in explaining differences among sites
  at this point; just want to account for their existence.
* Residuals from this model give an estimate of how richness changed over time
  at each site, accounting for observer differences.  We then used this "corrected"
  version of the data for our time-series models such as the AR1, etc.
* Note: throughout this manuscript, we use Gaussian errors for richness rather than 
  a proper count distribution because:
  * underdispersion (variance/mean << 1)
  * Easier to access out-of-the-box time series methods like auto-ARIMA.
  * Central limit theorem, large number of species (mean ~50 spp)

### Models

**Baseline models**
* The lme4 model fitted above.  This model assumes that the site-level average
  is constant, and tries to estimate its value.  The confidence intervals don't
  expand over time (except when observers change) because the model says nothing
  has changed
* Naive AR1 model (adjusted for observer effects). This model says that richness 
  can diffuse up or down from the previous year's value, without any constraints.
  As a result, the confidence intervals expand quickly over time.

**Time series models**
* Auto-ARIMA (Hyndman 2016). Choose a differencing order, then do forward stepwise
  model selection based on AIC.
* Auto-ARIMA + environment.

**SDMs**
* SDMs: use graf model w/ mixed model estimates of site \& observer effects
  as predictors (include error in predictors as estimated from the mixed model
  to propagate our uncertainty forward). This is basically SDM, but with a 
  better model (graf) and more information (monthly climate data instead of 
  using constant values from the Worldclim data, and observer effects). We used 
  Poisson-binomial confidence intervals, as discussed in Calabrese et al. 2014.
* JSDM. Should get best of the previous 2. Might be able to propagate uncertainty
  from lme4.
  
**"Macroecological" model**
* richness-level GBM w/ mixed model point estimates as bonus predictors where 
  available. This lets us predict richness directly instead of via noisy 
  species-level estimates, but doesn't use the full data set in one model.
  Maybe need to run multiple times to propagate uncertainty from lme4.
  
**Ensembles**
* Ensembles. Averaging predictions from multiple models tends to reduce the 
  noise in the estimates, and can thus lead to better predictions. Choosing the
  weights for the ensemble isn't straightforward because our estimates of model 
  error on the training set are biased in favor of models that
  overfit. [[Another option is to spatially cross-validate on the training set]]

**Evaluation**

Evaluated each year after the last training year (2010?):
* (R)MSE
* Log-likelihood (simultaneously rewards good point estimates, precision, and coverage)
* Coverage at 95%
* Calibration/bias

Looked at how error changed as the time horizon of forecasting lengthened
* Error will generally tend to increase
* Error might flatten out after some number of years
* Error might increase at different rates for different methods
  * One model might be better at short time scales \& worse at long time scales

## Results and conclusions

* None of the models actually work all that well, compared to baseline.
* Temporal richness variability doesn't track the predictors we use; sites have "inertia"
  * Cite 3 papers Ethan found on consistancy of alpha even when composition changes dramatically
  * Good for absolute accuracy, little room for improvement over baseline
  * Maybe richness is too coarse
    * abundance
    * species-level
* More important to use a reasonable error model than to get the best predictors \& basis expansions
* Ensembles help/don't help (depending on results)
* Point estimate error versus coverage
* In many ways, this is the best-case scenario for SDMs:
  * large data set, known \& consistent sampling technique over a period of decades
  * "True" absences
  * extended predictor set
  * birds arguably migrate quickly enough to make equlibrium \& especially space-for-time assumptions less ridiculous
  * errors for individual species can cancel out; only asking for aggregate values
* Value of stacking versus predicting the quantitity of interest directly
* Need to start doing this, even if we're bad at it.
  * We've posted predictions for the next N years of BBS data, using several methods, and invite others to do so as well.
* Dig around in the residuals for insights


## misc

SDM assumptions:
  * we've measured the predictive power of all avialable variables, other potentially important thing that aren't available are landcover, observer skill level, (probably some others)
  * Something like stationarity:
    * e.g. "systems are at equilibrium now and will be at equilibrium in future"
    * or "species track predictor variables quickly" (no lags)
    * etc
  * independent/identically distributed observations across species, years
  * Space substitutes well for time

None of the models are true forcasts as they use observed predictor variables (except the AR1). 


* Need predictions at multiple time scales
* Prediction quality at short time scales might not be a good indicator of 
  long-term effectiveness



## Box 1: Ten simple rules for making and evaluating ecological forecasts

### 1. Compare multiple modeling approaches

Typically ecological forecasts use one modeling approach or a small number of
related approaches. By fitting and evaluating multiple modeling approaches we
can learn more rapidly about the best approaches for making predictions for a
given ecological quantity. This includes comparing process based and data-driven
models.

### 2. Use time-series data when possible

Forecasts describe how systems are expected to change through time. While some
areas of ecological forecasting focus primarily on time-series data, others
primarily focus on using spatial models and space-for time substitutions. Using
ecological and environmental time-series data allows the consideration of actual
dynamics from both a process [@treddenick2016a] and error structure perspective.

### 3. Pay attention to uncertainty

Understanding confidence in a forecast is just as important as understanding the
average or expected outcome. Failing to account for uncertainty can result in
overconfidence in highly uncertain outcomes leading to poor decision making and
erosion of confidence in ecological forecasts. Models should explicitly include
sources of uncertainty where possible and evaluations of forecasts should assess
the accuracy of uncertainties as well as point estimates [@dietz2017].

### 4. Use predictors related to the question

Many ecological forecasts use data that is readily available and easy to work
with. While ease of use is a reasonable consideration it is also important to
include predictor variables that are expected to relate to the ecological
quantity being forecast and dynamic time-series of predictors instead of
long-term averages. Investing time in identifying and acquiring better predictor
variables may have at least as many benefits as using more sophisticated
modeling techniques.

### 5. Assess how forecast accuracy changes with time-lag

In general the accuracy of forecasts decreases with the length of time into the
future being forecast [@petch2015]. This decay in accuracy and associated
forecast horizons should be considered when evaluating forecasts and comparing
models.

### 6. Include an observation model

Ecological observations are influenced by both the underlying processes and how
the system is sampled. When possible forecasts should model the factors
influencing the observation of the data.

### 7. Validate using hindcasting

To evaluate the expected accuracy and uncertainty of forecasts assess the
performance of these forecasts within existing time-series data.

### 8. Publicly archive forecasts

To allow the future evaluation of the accuracy and uncertainty of forecasts the
forecast values and/or models should be archived so that they can be assessed
after new data is generated [@mcgill2012]. Enough information should be provided
to allow an unambiguous assessment of the forecast performance.

### 9. Make short-term and long-term predictions

In cases where long-term predictions are the primary goal, short-term should
also be made to accommodate the time-scales of planning and management decisions
and to allow the accuracy of the forecasts to be quickly evaluated
[@treddenick2016a];

### 10. Compare forecasts to simple baselines

Compare the accuracy of the forecasts to simple baselines such as the
time-series average to determine if the modeled forecasts are more accurate than
the naive assumption that the world is static.



