# Moving towards large-scale data-intensive forecasting of species richness in breeding birds

Authors: Harris, Taylor and White?

## Introduction

Forecasting the future state of ecological systems is increasingly important for
planning, management, and testing the performance of ecological models
[@clark2001, @dietz2017]. One area of particular interest for forecasting is
biodiversity since it is important for ecosystem function, central to
conservation planning, and expected to be influenced by anthropogenic global
change. Anticipating potential changes in biodiversity is crucial for large
scale management and conservation, and for addressing debates regarding whether
local scale diversity is declining and may continue to decline in response to
anthropogenic influences [@dornelas2014].

Previous efforts to predict future patterns of species richness, and diversity
more generally, have focused primarily on building species distributions models
to quantify the spatial relationships between the occurrence of individual
species and the environment. Forecasts for future environmental conditions are
then used to predict where each species will occur in the future and the
probabilities of occurrence are summed across species to predict future species
richness [e.g., @thomas2004]. Alternatively, models that directly relate spatial
patterns of species richness to the environment have been developed and
generally perform equivalently to species distribution modeling based methods
[@algar2009, @distler2015].

However, despite the emerging interest in forecasting species richness and other
aspects of biodiversity, little is known about how effectively we can anticipate
their dynamics. What we do know comes from a small number of hindcasting
studies, where models are built using data on species occurrence and richness
from the past and evaluated on their ability to predict contemporary patterns
[e.g., @algar2009, @distler2015]. These studies are a valuable first step, but
lack several components that are important for developing good forecasting
models and understanding how accurately these models can predict the
future. These components of good forecasting and evaluation (Box 1) broadly
involve: 1) expanding the use of data to include biological and environmental
time-series [@treddenick2016]; 2) accounting for uncertainty in observations,
processes, model choice, and forecast evaluation [@dietz2017]; and 3) conducting
meaningful evaluations of the forecasts by hindcasting, archiving short-term
forecasts, and comparing forecasts to baselines to determine if the forecasts
are more accurate than assuming the system is basically static [@Perretti2013].

In this paper we attempt to forecast the species richness of breeding birds at
hundreds of locations throughout North America while following the best
practices in Box 1 for ecological forecasting. To do this we combine 30 years of
time-series data on bird distributions with monthly time-series of climate data
and satellite-based remote-sensing. We compare traditional distribution modeling
based approaches to spatial models of species richness, time-series methods, and
simple baselines. All forecasts model uncertainty and observation, are evaluated
across different time lags using hindcasting, and are publicly archived to allow
future assessment. We discuss the implications of these practices for our
understanding of, and confidence in, the resulting forecasts, and how we can
continue to improve on these approaches in the future.


## Methods

### Data

* BBS + retriever
* Monthly climate data from interpolated weather stations (PRISM)
* NDVI (GIMMS)
* Altitude (SRTM 90m Digital Elevation Database via raster::getData)

### Accounting for site \& observer effects
* Observer effects are inherent in
  large data sets taken by different observers, and are known to occur in BBS 
  [citation].
* Mixed model for observer effort \& site effects: used Stan, included random
  effects observer_id to account for situations were one observer consistently
  found more (or fewer) species than average. These models also included a 
  site-level random effect to account for differences in actual richness across 
  BBS routes. [[Maybe year effect?]].
  No fixed effects because not interested in explaining differences among sites
  at this point; just want to account for their existence.
* Residuals from this model give an estimate of how richness changed over time
  at each site, accounting for observer differences.  We then used this "corrected"
  version of the data for our time-series models such as the AR1, etc.
* Note: throughout this manuscript, we use Gaussian errors for richness rather than 
  a proper count distribution because:
  * underdispersion (variance/mean << 1)
  * Easier to access out-of-the-box time series methods like auto-ARIMA.
  * Central limit theorem, large number of species (mean ~50 spp)

### Models

**Baseline models**
* The lme4 model fitted above.  This model assumes that the site-level average
  is constant, and tries to estimate its value.  The confidence intervals don't
  expand over time (except when observers change) because the model says nothing
  has changed
* Naive AR1 model (adjusted for observer effects). This model says that richness 
  can diffuse up or down from the previous year's value, without any constraints.
  As a result, the confidence intervals expand quickly over time.

**Time series models**
* Auto-ARIMA (Hyndman 2016). Choose a differencing order, then do forward stepwise
  model selection based on AIC.
* Auto-ARIMA + environment.

**SDMs**
* SDMs: use graf model w/ mixed model estimates of site \& observer effects
  as predictors (include error in predictors as estimated from the mixed model
  to propagate our uncertainty forward). This is basically SDM, but with a 
  better model (graf) and more information (monthly climate data instead of 
  using constant values from the Worldclim data, and observer effects). We used 
  Poisson-binomial confidence intervals, as discussed in Calabrese et al. 2014.
* JSDM. Should get best of the previous 2. Might be able to propagate uncertainty
  from lme4.
  
**"Macroecological" model**
* richness-level GBM w/ mixed model point estimates as bonus predictors where 
  available. This lets us predict richness directly instead of via noisy 
  species-level estimates, but doesn't use the full data set in one model.
  Maybe need to run multiple times to propagate uncertainty from lme4.
  
**Ensembles**
* Ensembles. Averaging predictions from multiple models tends to reduce the 
  noise in the estimates, and can thus lead to better predictions. Choosing the
  weights for the ensemble isn't straightforward because our estimates of model 
  error on the training set are biased in favor of models that
  overfit. [[Another option is to spatially cross-validate on the training set]]

**Evaluation**

Evaluated each year after the last training year (2010?):
* (R)MSE
* Log-likelihood (simultaneously rewards good point estimates, precision, and coverage)
* Coverage at 95%
* Calibration/bias

Looked at how error changed as the time horizon of forecasting lengthened
* Error will generally tend to increase
* Error might flatten out after some number of years
* Error might increase at different rates for different methods
  * One model might be better at short time scales \& worse at long time scales

## Results and conclusions

* None of the models actually work all that well, compared to baseline.
* Temporal richness variability doesn't track the predictors we use; sites have "inertia"
  * Cite 3 papers Ethan found on consistancy of alpha even when composition changes dramatically
  * Good for absolute accuracy, little room for improvement over baseline
  * Maybe richness is too coarse
    * abundance
    * species-level
* More important to use a reasonable error model than to get the best predictors \& basis expansions
* Ensembles help/don't help (depending on results)
* Point estimate error versus coverage
* In many ways, this is the best-case scenario for SDMs:
  * large data set, known \& consistent sampling technique over a period of decades
  * "True" absences
  * extended predictor set
  * birds arguably migrate quickly enough to make equlibrium \& especially space-for-time assumptions less ridiculous
  * errors for individual species can cancel out; only asking for aggregate values
* Value of stacking versus predicting the quantitity of interest directly
* Need to start doing this, even if we're bad at it.
  * We've posted predictions for the next N years of BBS data, using several methods, and invite others to do so as well.
* Dig around in the residuals for insights


## misc

SDM assumptions:
  * we've measured the predictive power of all avialable variables, other potentially important thing that aren't available are landcover, observer skill level, (probably some others)
  * Something like stationarity:
    * e.g. "systems are at equilibrium now and will be at equilibrium in future"
    * or "species track predictor variables quickly" (no lags)
    * etc
  * independent/identically distributed observations across species, years
  * Space substitutes well for time

None of the models are true forcasts as they use observed predictor variables (except the AR1). 


* Need predictions at multiple time scales
* Prediction quality at short time scales might not be a good indicator of 
  long-term effectiveness



## Box 1: Ten simple rules for making and evaluating ecological forecasts

### 1. Compare multiple modeling approaches

Typically ecological forecasts use one modeling approach or a small number of
related approaches. By fitting and evaluating multiple modeling approaches we
can learn more rapidly about the best approaches for making predictions for a
given ecological quantity. This includes comparing process based and data-driven
models.

### 2. Use time-series data when possible

Forecasts describe how systems are expected to change through time. While some
areas of ecological forecasting focus primarily on time-series data, others
primarily focus on using spatial models and space-for time substitutions. Using
ecological and environmental time-series data allows the consideration of actual
dynamics from both a process [@treddenick2016a] and error structure perspective.

### 3. Pay attention to uncertainty

Understanding confidence in a forecast is just as important as understanding the
average or expected outcome. Failing to account for uncertainty can result in
overconfidence in highly uncertain outcomes leading to poor decision making and
erosion of confidence in ecological forecasts. Models should explicitly include
sources of uncertainty where possible and evaluations of forecasts should assess
the accuracy of uncertainties as well as point estimates [@dietz2017].

### 4. Use predictors related to the question

Many ecological forecasts use data that is readily available and easy to work
with. While ease of use is a reasonable consideration it is also important to
include predictor variables that are expected to relate to the ecological
quantity being forecast and dynamic time-series of predictors instead of
long-term averages. Investing time in identifying and acquiring better predictor
variables may have at least as many benefits as using more sophisticated
modeling techniques.

### 5. Assess how forecast accuracy changes with time-lag

In general the accuracy of forecasts decreases with the length of time into the
future being forecast [@petch2015]. This decay in accuracy and associated
forecast horizons should be considered when evaluating forecasts and comparing
models.

### 6. Include an observation model

Ecological observations are influenced by both the underlying processes and how
the system is sampled. When possible forecasts should model the factors
influencing the observation of the data.

### 7. Validate using hindcasting

To evaluate the expected accuracy and uncertainty of forecasts assess the
performance of these forecasts within existing time-series data.

### 8. Publicly archive forecasts

To allow the future evaluation of the accuracy and uncertainty of forecasts the
forecast values and/or models should be archived so that they can be assessed
after new data is generated [@mcgill2012]. Enough information should be provided
to allow an unambiguous assessment of the forecast performance.

### 9. Make short-term and long-term predictions

In cases where long-term predictions are the primary goal, short-term should
also be made to accommodate the time-scales of planning and management decisions
and to allow the accuracy of the forecasts to be quickly evaluated
[@treddenick2016a];

### 10. Compare forecasts to simple baselines

Compare the accuracy of the forecasts to simple baselines such as the
time-series average to determine if the modeled forecasts are more accurate than
the naive assumption that the world is static.



